{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ_sKev24dLT",
        "outputId": "48988fca-5ea2-4c7b-aea0-4c12e3ed0cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 40 (delta 5), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (40/40), 441.76 KiB | 3.88 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HEMASAGAR07/MainDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1P-N3N04hVl",
        "outputId": "074977bc-c5f1-4945-f004-be2554ae1819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Cloning into 'IndicTrans2'...\n",
            "remote: Enumerating objects: 741, done.\u001b[K\n",
            "remote: Counting objects: 100% (169/169), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 741 (delta 140), reused 105 (delta 102), pack-reused 572 (from 1)\u001b[K\n",
            "Receiving objects: 100% (741/741), 4.14 MiB | 6.12 MiB/s, done.\n",
            "Resolving deltas: 100% (482/482), done.\n",
            "/content/IndicTrans2/huggingface_interface/IndicTrans2/huggingface_interface\n",
            "Setting up the environment in the /content/IndicTrans2/huggingface_interface/IndicTrans2/huggingface_interface\n",
            "Creating a virtual environment with python3\n",
            "install.sh: line 10: conda: command not found\n",
            "install.sh: line 11: conda: command not found\n",
            "Installing all the dependencies\n",
            "install.sh: line 14: conda: command not found\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Cloning into 'IndicTransToolkit'...\n",
            "remote: Enumerating objects: 155, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 155 (delta 34), reused 45 (delta 30), pack-reused 95 (from 1)\u001b[K\n",
            "Receiving objects: 100% (155/155), 3.88 MiB | 19.64 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "Obtaining file:///content/IndicTrans2/huggingface_interface/IndicTrans2/huggingface_interface/IndicTransToolkit\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library (from IndicTransToolkit==1.0.2)\n",
            "  Cloning https://github.com/VarunGumma/indic_nlp_library to /tmp/pip-install-6qitkqg7/indic-nlp-library-it2_5b3c5c957dc74c3da165ffa25c142a87\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/VarunGumma/indic_nlp_library /tmp/pip-install-6qitkqg7/indic-nlp-library-it2_5b3c5c957dc74c3da165ffa25c142a87\n",
            "  Resolved https://github.com/VarunGumma/indic_nlp_library to commit 601521e05ed0ed8f2165ac317a47d186e25b6f0d\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.2) (75.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.2) (2.5.0+cu121)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.2) (0.1.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.2) (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.2) (4.44.2)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (from IndicTransToolkit==1.0.2) (2.4.3)\n",
            "Requirement already satisfied: sphinx-argparse in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (0.5.2)\n",
            "Requirement already satisfied: sphinx_rtd_theme in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (3.0.1)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (1.26.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.2) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.2) (2024.9.11)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu->IndicTransToolkit==1.0.2) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransToolkit==1.0.2) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransToolkit==1.0.2) (1.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransToolkit==1.0.2) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.2) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.2) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.2) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransToolkit==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->IndicTransToolkit==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.2) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.2) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.2) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransToolkit==1.0.2) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->IndicTransToolkit==1.0.2) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransToolkit==1.0.2) (2024.8.30)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (8.1.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (0.21.2)\n",
            "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in /usr/local/lib/python3.10/dist-packages (from sphinx_rtd_theme->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.0.0)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.18.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.16.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (0.7.16)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (1.4.1)\n",
            "Requirement already satisfied: tomli>=2 in /usr/local/lib/python3.10/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransToolkit==1.0.2) (2.0.2)\n",
            "Installing collected packages: IndicTransToolkit\n",
            "  Attempting uninstall: IndicTransToolkit\n",
            "    Found existing installation: IndicTransToolkit 1.0.2\n",
            "    Uninstalling IndicTransToolkit-1.0.2:\n",
            "      Successfully uninstalled IndicTransToolkit-1.0.2\n",
            "\u001b[33m  DEPRECATION: Legacy editable install of IndicTransToolkit==1.0.2 from file:///content/IndicTrans2/huggingface_interface/IndicTrans2/huggingface_interface/IndicTransToolkit (setup.py develop) is deprecated. pip 25.0 will enforce this behaviour change. A possible replacement is to add a pyproject.toml or enable --use-pep517, and use setuptools >= 64. If the resulting installation is not behaving as expected, try using --config-settings editable_mode=compat. Please consult the setuptools documentation for more information. Discussion can be found at https://github.com/pypa/pip/issues/11457\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py develop for IndicTransToolkit\n",
            "Successfully installed IndicTransToolkit\n",
            "Setup completed!\n"
          ]
        }
      ],
      "source": [
        "!pip install peft\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2\n",
        "%cd IndicTrans2/huggingface_interface\n",
        "!source install.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vf2rQZ94YDJ",
        "outputId": "f37a6ed9-692b-4ea8-b7b7-d37a808f4326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-20 08:21:58.667920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-20 08:21:58.689917: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-20 08:21:58.697107: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-20 08:21:58.712214: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-20 08:21:59.974934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            " | > Loading ai4bharat/indictrans2-en-indic-dist-200M and tokenizer ...\n",
            "Map (num_proc=8):   0% 0/997 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  13% 125/997 [00:01<00:12, 70.48 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  25% 250/997 [00:04<00:13, 56.62 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  38% 375/997 [00:06<00:12, 51.43 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  50% 500/997 [00:09<00:10, 48.14 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  63% 625/997 [00:12<00:08, 45.98 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  75% 749/997 [00:14<00:04, 50.89 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  88% 873/997 [00:16<00:02, 57.84 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8): 100% 997/997 [00:17<00:00, 56.30 examples/s]\n",
            " | > Loaded train dataset from /content/dataset/en-indic-exp. Size: 997 ...\n",
            "Map (num_proc=8):   0% 0/1012 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  13% 127/1012 [00:01<00:12, 70.58 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  25% 254/1012 [00:03<00:09, 76.24 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  38% 381/1012 [00:05<00:10, 60.64 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  50% 508/1012 [00:08<00:09, 53.79 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  63% 634/1012 [00:11<00:07, 49.15 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  75% 760/1012 [00:14<00:05, 48.00 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  88% 886/1012 [00:16<00:02, 52.41 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8): 100% 1012/1012 [00:17<00:00, 56.49 examples/s]\n",
            " | > Loaded eval dataset from /content/dataset/en-indic-exp. Size: 1012 ...\n",
            "trainable params: 1,769,472 || all params: 276,354,048 || trainable%: 0.6403\n",
            " | > Loading metrics factory with BLEU and chrF ...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            " | > Starting training ...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 1.4512, 'grad_norm': 0.1461333930492401, 'learning_rate': 5e-06, 'epoch': 12.5, 'num_input_tokens_seen': 506112}\n",
            "{'loss': 1.4384, 'grad_norm': 0.2643783390522003, 'learning_rate': 1e-05, 'epoch': 25.0, 'num_input_tokens_seen': 1010944}\n",
            "{'loss': 1.4062, 'grad_norm': 0.1334010362625122, 'learning_rate': 1.5e-05, 'epoch': 37.5, 'num_input_tokens_seen': 1518632}\n",
            "{'loss': 1.3609, 'grad_norm': 0.21146024763584137, 'learning_rate': 2e-05, 'epoch': 50.0, 'num_input_tokens_seen': 2021280}\n",
            "{'loss': 1.3116, 'grad_norm': 0.12253718078136444, 'learning_rate': 2.5e-05, 'epoch': 62.5, 'num_input_tokens_seen': 2527136}\n",
            "{'loss': 1.2793, 'grad_norm': 0.24692398309707642, 'learning_rate': 3e-05, 'epoch': 75.0, 'num_input_tokens_seen': 3031064}\n",
            "{'loss': 1.2414, 'grad_norm': 0.1463450789451599, 'learning_rate': 3.5e-05, 'epoch': 87.5, 'num_input_tokens_seen': 3537472}\n",
            "{'loss': 1.1962, 'grad_norm': 0.2999316453933716, 'learning_rate': 4e-05, 'epoch': 100.0, 'num_input_tokens_seen': 4041520}\n",
            "{'loss': 1.1466, 'grad_norm': 0.16796472668647766, 'learning_rate': 4.5e-05, 'epoch': 112.5, 'num_input_tokens_seen': 4548184}\n",
            "{'loss': 1.0883, 'grad_norm': 0.37997812032699585, 'learning_rate': 5e-05, 'epoch': 125.0, 'num_input_tokens_seen': 5054456}\n",
            "100% 1000/1000 [19:48<00:00,  1.02s/it]\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:03<00:53,  1.78s/it]\u001b[A\n",
            "  9% 3/32 [00:09<01:38,  3.40s/it]\u001b[A\n",
            " 12% 4/32 [00:14<01:56,  4.15s/it]\u001b[A\n",
            " 16% 5/32 [00:20<02:08,  4.75s/it]\u001b[A\n",
            " 19% 6/32 [00:42<04:32, 10.46s/it]\u001b[A\n",
            " 22% 7/32 [00:49<03:49,  9.20s/it]\u001b[A\n",
            " 25% 8/32 [00:52<02:59,  7.46s/it]\u001b[A\n",
            " 28% 9/32 [01:14<04:36, 12.00s/it]\u001b[A\n",
            " 31% 10/32 [01:18<03:24,  9.32s/it]\u001b[A\n",
            " 34% 11/32 [01:21<02:40,  7.64s/it]\u001b[A\n",
            " 38% 12/32 [01:43<03:56, 11.85s/it]\u001b[A\n",
            " 41% 13/32 [02:05<04:42, 14.86s/it]\u001b[A\n",
            " 44% 14/32 [02:09<03:30, 11.68s/it]\u001b[A\n",
            " 47% 15/32 [02:12<02:35,  9.13s/it]\u001b[A\n",
            " 50% 16/32 [02:34<03:25, 12.84s/it]\u001b[A\n",
            " 53% 17/32 [02:39<02:37, 10.51s/it]\u001b[A\n",
            " 56% 18/32 [03:01<03:14, 13.90s/it]\u001b[A\n",
            " 59% 19/32 [03:21<03:27, 15.92s/it]\u001b[A\n",
            " 62% 20/32 [03:43<03:30, 17.57s/it]\u001b[A\n",
            " 66% 21/32 [04:07<03:34, 19.52s/it]\u001b[A\n",
            " 69% 22/32 [04:13<02:36, 15.64s/it]\u001b[A\n",
            " 72% 23/32 [04:35<02:37, 17.50s/it]\u001b[A\n",
            " 75% 24/32 [04:56<02:28, 18.52s/it]\u001b[A\n",
            " 78% 25/32 [05:04<01:46, 15.23s/it]\u001b[A\n",
            " 81% 26/32 [05:08<01:12, 12.10s/it]\u001b[A\n",
            " 84% 27/32 [05:12<00:48,  9.61s/it]\u001b[A\n",
            " 88% 28/32 [05:34<00:53, 13.30s/it]\u001b[A\n",
            " 91% 29/32 [05:58<00:49, 16.60s/it]\u001b[A\n",
            " 94% 30/32 [06:21<00:36, 18.33s/it]\u001b[A\n",
            " 97% 31/32 [06:42<00:19, 19.24s/it]\u001b[A\n",
            "100% 32/32 [06:47<00:00, 14.88s/it]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.31119966506958, 'eval_BLEU': 22.52421322502027, 'eval_chrF': 61.35239491053831, 'eval_runtime': 434.4603, 'eval_samples_per_second': 2.329, 'eval_steps_per_second': 0.074, 'epoch': 125.0, 'num_input_tokens_seen': 5054456}\n",
            "100% 1000/1000 [27:03<00:00,  1.02s/it]\n",
            "100% 32/32 [06:53<00:00, 14.88s/it]\u001b[A\n",
            "{'train_runtime': 1623.9156, 'train_samples_per_second': 78.822, 'train_steps_per_second': 0.616, 'train_tokens_per_second': 6305.746, 'train_loss': 1.2919972534179687, 'epoch': 125.0, 'num_input_tokens_seen': 5054456}\n",
            "100% 1000/1000 [27:03<00:00,  1.62s/it]\n"
          ]
        }
      ],
      "source": [
        "!python train_lora.py \\\n",
        "    --model \"ai4bharat/indictrans2-en-indic-dist-200M\" \\\n",
        "    --src_lang_list \"eng_Latn\" \\\n",
        "    --tgt_lang_list \"tel_Telu\" \\\n",
        "    --data_dir \"/content/dataset/en-indic-exp\" \\\n",
        "    --output_dir \"/content/fine_tuned_model_telugu\" \\\n",
        "    --save_steps 1000 \\\n",
        "    --max_steps 1000 \\\n",
        "    --batch_size 32 \\\n",
        "    --grad_accum_steps 4 \\\n",
        "    --warmup_steps 4000 \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --adam_beta1 0.9 \\\n",
        "    --adam_beta2 0.98 \\\n",
        "    --optimizer adamw_torch \\\n",
        "    --lr_scheduler inverse_sqrt \\\n",
        "    --num_workers 16 \\\n",
        "    --metric_for_best_model eval_BLEU \\\n",
        "    --greater_is_better \\\n",
        "    --patience 10 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --lora_target_modules \"q_proj,k_proj\" \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --lora_r 16 \\\n",
        "    --lora_alpha 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EQs05pa68Zf",
        "outputId": "f33b6f72-262c-4a91-a0b5-5c3f98838036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-10-20 07:58:46.292591: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-20 07:58:46.394340: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-20 07:58:46.404541: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-20 07:58:46.428098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-20 07:58:48.736895: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            " | > Loading ai4bharat/indictrans2-en-indic-dist-200M and tokenizer ...\n",
            "Map (num_proc=8):   0% 0/997 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  13% 125/997 [00:01<00:12, 70.24 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  25% 250/997 [00:03<00:09, 76.19 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  38% 375/997 [00:04<00:07, 78.88 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  50% 500/997 [00:06<00:06, 79.90 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  63% 625/997 [00:07<00:04, 80.37 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  75% 749/997 [00:09<00:03, 78.44 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  88% 873/997 [00:12<00:01, 62.16 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8): 100% 997/997 [00:15<00:00, 66.24 examples/s]\n",
            " | > Loaded train dataset from /content/dataset/en-indic-exp. Size: 997 ...\n",
            "Map (num_proc=8):   0% 0/1012 [00:00<?, ? examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  13% 127/1012 [00:01<00:12, 68.95 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  25% 254/1012 [00:03<00:09, 76.76 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  38% 381/1012 [00:04<00:07, 79.65 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  50% 508/1012 [00:06<00:06, 80.85 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  63% 634/1012 [00:07<00:04, 80.73 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  75% 760/1012 [00:10<00:03, 64.48 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8):  88% 886/1012 [00:13<00:02, 55.35 examples/s]/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Map (num_proc=8): 100% 1012/1012 [00:16<00:00, 62.16 examples/s]\n",
            " | > Loaded eval dataset from /content/dataset/en-indic-exp. Size: 1012 ...\n",
            "trainable params: 1,769,472 || all params: 276,354,048 || trainable%: 0.6403\n",
            " | > Loading metrics factory with BLEU and chrF ...\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            " | > Starting training ...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "{'loss': 1.598, 'grad_norm': 0.1429300159215927, 'learning_rate': 5e-06, 'epoch': 12.5, 'num_input_tokens_seen': 506112}\n",
            "{'loss': 1.5867, 'grad_norm': 0.206959530711174, 'learning_rate': 1e-05, 'epoch': 25.0, 'num_input_tokens_seen': 1010944}\n",
            "{'loss': 1.554, 'grad_norm': 0.11741670966148376, 'learning_rate': 1.5e-05, 'epoch': 37.5, 'num_input_tokens_seen': 1518632}\n",
            "{'loss': 1.5099, 'grad_norm': 0.25469234585762024, 'learning_rate': 2e-05, 'epoch': 50.0, 'num_input_tokens_seen': 2021280}\n",
            "{'loss': 1.4711, 'grad_norm': 0.13271380960941315, 'learning_rate': 2.5e-05, 'epoch': 62.5, 'num_input_tokens_seen': 2527136}\n",
            "{'loss': 1.4393, 'grad_norm': 0.2385345697402954, 'learning_rate': 3e-05, 'epoch': 75.0, 'num_input_tokens_seen': 3031064}\n",
            "{'loss': 1.3952, 'grad_norm': 0.1635633260011673, 'learning_rate': 3.5e-05, 'epoch': 87.5, 'num_input_tokens_seen': 3537472}\n",
            "{'loss': 1.3459, 'grad_norm': 0.2771916389465332, 'learning_rate': 4e-05, 'epoch': 100.0, 'num_input_tokens_seen': 4041520}\n",
            "{'loss': 1.2915, 'grad_norm': 0.2055366337299347, 'learning_rate': 4.5e-05, 'epoch': 112.5, 'num_input_tokens_seen': 4548184}\n",
            "{'loss': 1.2324, 'grad_norm': 0.34187573194503784, 'learning_rate': 5e-05, 'epoch': 125.0, 'num_input_tokens_seen': 5054456}\n",
            "100% 1000/1000 [19:13<00:00,  1.29s/it]\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/32 [00:03<00:58,  1.94s/it]\u001b[A\n",
            "  9% 3/32 [00:10<01:53,  3.92s/it]\u001b[A\n",
            " 12% 4/32 [00:15<02:05,  4.47s/it]\u001b[A\n",
            " 16% 5/32 [00:22<02:16,  5.06s/it]\u001b[A\n",
            " 19% 6/32 [00:26<02:06,  4.88s/it]\u001b[A\n",
            " 22% 7/32 [00:32<02:07,  5.09s/it]\u001b[A\n",
            " 25% 8/32 [00:36<01:56,  4.85s/it]\u001b[A\n",
            " 28% 9/32 [00:41<01:52,  4.91s/it]\u001b[A\n",
            " 31% 10/32 [00:47<01:53,  5.17s/it]\u001b[A\n",
            " 34% 11/32 [00:51<01:45,  5.02s/it]\u001b[A\n",
            " 38% 12/32 [00:55<01:30,  4.52s/it]\u001b[A\n",
            " 41% 13/32 [00:59<01:21,  4.30s/it]\u001b[A\n",
            " 44% 14/32 [01:03<01:19,  4.42s/it]\u001b[A\n",
            " 47% 15/32 [01:08<01:15,  4.43s/it]\u001b[A\n",
            " 50% 16/32 [01:13<01:15,  4.70s/it]\u001b[A\n",
            " 53% 17/32 [01:19<01:16,  5.09s/it]\u001b[A\n",
            " 56% 18/32 [01:24<01:09,  4.94s/it]\u001b[A\n",
            " 59% 19/32 [01:45<02:07,  9.82s/it]\u001b[A\n",
            " 62% 20/32 [01:49<01:36,  8.06s/it]\u001b[A\n",
            " 66% 21/32 [01:55<01:20,  7.35s/it]\u001b[A\n",
            " 69% 22/32 [02:01<01:11,  7.18s/it]\u001b[A\n",
            " 72% 23/32 [02:06<00:58,  6.47s/it]\u001b[A\n",
            " 75% 24/32 [02:12<00:49,  6.18s/it]\u001b[A\n",
            " 78% 25/32 [02:18<00:44,  6.36s/it]\u001b[A\n",
            " 81% 26/32 [02:23<00:34,  5.69s/it]\u001b[A\n",
            " 84% 27/32 [02:27<00:27,  5.40s/it]\u001b[A\n",
            " 88% 28/32 [02:33<00:21,  5.42s/it]\u001b[A\n",
            " 91% 29/32 [02:38<00:16,  5.48s/it]\u001b[A\n",
            " 94% 30/32 [02:42<00:09,  4.96s/it]\u001b[A\n",
            " 97% 31/32 [02:47<00:04,  4.81s/it]\u001b[A\n",
            "100% 32/32 [02:52<00:00,  4.93s/it]\u001b[A/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.6123846769332886, 'eval_BLEU': 21.80903376307036, 'eval_chrF': 61.7322538885386, 'eval_runtime': 181.8748, 'eval_samples_per_second': 5.564, 'eval_steps_per_second': 0.176, 'epoch': 125.0, 'num_input_tokens_seen': 5054456}\n",
            "100% 1000/1000 [22:14<00:00,  1.29s/it]\n",
            "100% 32/32 [02:57<00:00,  4.93s/it]\u001b[A\n",
            "{'train_runtime': 1335.3996, 'train_samples_per_second': 95.851, 'train_steps_per_second': 0.749, 'train_tokens_per_second': 7668.116, 'train_loss': 1.4424105453491212, 'epoch': 125.0, 'num_input_tokens_seen': 5054456}\n",
            "100% 1000/1000 [22:15<00:00,  1.34s/it]\n"
          ]
        }
      ],
      "source": [
        "!python train_lora.py \\\n",
        "    --model \"ai4bharat/indictrans2-en-indic-dist-200M\" \\\n",
        "    --src_lang_list \"eng_Latn\" \\\n",
        "    --tgt_lang_list \"tam_Taml\" \\\n",
        "    --data_dir \"/content/dataset/en-indic-exp\" \\\n",
        "    --output_dir \"/content/fine_tuned_model_tamil\" \\\n",
        "    --save_steps 1000 \\\n",
        "    --max_steps 1000 \\\n",
        "    --batch_size 32 \\\n",
        "    --grad_accum_steps 4 \\\n",
        "    --warmup_steps 4000 \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --adam_beta1 0.9 \\\n",
        "    --adam_beta2 0.98 \\\n",
        "    --optimizer adamw_torch \\\n",
        "    --lr_scheduler inverse_sqrt \\\n",
        "    --num_workers 16 \\\n",
        "    --metric_for_best_model eval_BLEU \\\n",
        "    --greater_is_better \\\n",
        "    --patience 10 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --lora_target_modules \"q_proj,k_proj\" \\\n",
        "    --lora_dropout 0.1 \\\n",
        "    --lora_r 16 \\\n",
        "    --lora_alpha 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PGAAIXapvLnR",
        "outputId": "620151ff-cb3b-4076-9efb-e35548d78aa4"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_92cc23e3-6545-4f23-8606-2ab99502ee0b\", \"fine_tuned_model_telugu.zip\", 26264556)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_24edfd74-6098-43f2-a59a-ca9e40d27147\", \"fine_tuned_model_tamil.zip\", 26267707)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Specify the folder path and the desired name for the zip file\n",
        "folder_path = '/content/fine_tuned_model_telugu'  # Replace with your folder path\n",
        "zip_file_name = 'fine_tuned_model_telugu.zip'  # Replace with the desired zip file name\n",
        "\n",
        "# Create a zip file from the folder\n",
        "shutil.make_archive(zip_file_name[:-4], 'zip', folder_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_name)\n",
        "\n",
        "folder_path = '/content/fine_tuned_model_tamil'  # Replace with your folder path\n",
        "zip_file_name = 'fine_tuned_model_tamil.zip'  # Replace with the desired zip file name\n",
        "\n",
        "# Create a zip file from the folder\n",
        "shutil.make_archive(zip_file_name[:-4], 'zip', folder_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9M-r7oiJ_vW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UuWRfdBKDEq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqt8hpHMKEoh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd ..\n",
        "\n",
        "#restart session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb_xPavSKGrA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBMA5lBjKJZE"
      },
      "outputs": [],
      "source": [
        "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
        "    if quantization == \"4-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    elif quantization == \"8-bit\":\n",
        "        qconfig = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_use_double_quant=True,\n",
        "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "    else:\n",
        "        qconfig = None\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        ckpt_dir,\n",
        "        trust_remote_code=True,\n",
        "        low_cpu_mem_usage=True,\n",
        "        quantization_config=qconfig,\n",
        "    )\n",
        "\n",
        "    if qconfig == None:\n",
        "        model = model.to(DEVICE)\n",
        "        if DEVICE == \"cuda\":\n",
        "            model.half()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return tokenizer, model\n",
        "\n",
        "\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            generated_tokens = tokenizer.batch_decode(\n",
        "                generated_tokens.detach().cpu().tolist(),\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxOQTaVuL2SW",
        "outputId": "e73cadf9-7352-4fb0-c523-af9686572038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files have been unzipped to: fine_tuned_model_telugu\n",
            "Files have been unzipped to: fine_tuned_model_tamil\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/fine_tuned_model_telugu.zip'\n",
        "\n",
        "# Path to extract the files\n",
        "extract_to_path = 'fine_tuned_model_telugu'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_to_path, exist_ok=True)\n",
        "\n",
        "# Unzipping the folder\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "print(f\"Files have been unzipped to: {extract_to_path}\")\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/fine_tuned_model_tamil.zip'\n",
        "\n",
        "# Path to extract the files\n",
        "extract_to_path = 'fine_tuned_model_tamil'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_to_path, exist_ok=True)\n",
        "\n",
        "# Unzipping the folder\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "print(f\"Files have been unzipped to: {extract_to_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmBoqVtUK9oU"
      },
      "outputs": [],
      "source": [
        "# Install required libraries if not already installed\n",
        "!pip install transformers peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7cOiDfh49xP",
        "outputId": "73286344-84e5-4872-b8c2-e30bf517b6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source: When I was young, I used to go to the park every day.\n",
            "Translation: నేను చిన్నప్పుడు, ప్రతిరోజూ పార్కుకు వెళ్లేవాడిని. \n",
            "\n",
            "Source: We watched a new movie last week, which was very inspiring.\n",
            "Translation: మేము గత వారం ఒక కొత్త సినిమా చూశాము, ఇది చాలా స్ఫూర్తిదాయకంగా ఉంది. \n",
            "\n",
            "Source: If you had met me at that time, we would have gone out to eat.\n",
            "Translation: ఆ సమయంలో మీరు నన్ను కలుసుకున్నట్లయితే, మేము తినడానికి బయటకు వెళ్లి ఉండేవాళ్లం. \n",
            "\n",
            "Source: My friend has invited me to his birthday party, and I will give him a gift.\n",
            "Translation: నా స్నేహితుడు నన్ను తన పుట్టినరోజు పార్టీకి ఆహ్వానించాడు, నేను అతనికి ఒక బహుమతి ఇస్తాను. \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "\n",
        "# Load the base model\n",
        "base_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # Change this as per your use case\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_ckpt_dir, trust_remote_code=True)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_ckpt_dir, trust_remote_code=True)\n",
        "\n",
        "# Load the LoRA model\n",
        "lora_ckpt_dir = \"/content/fine_tuned_model_telugu\"  # Path to your fine-tuned model directory\n",
        "lora_model = PeftModel.from_pretrained(base_model, lora_ckpt_dir)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lora_model.to(DEVICE)\n",
        "\n",
        "# Test translation with your model\n",
        "input_sentences = [\n",
        "    \"When I was young, I used to go to the park every day.\",\n",
        "    \"We watched a new movie last week, which was very inspiring.\",\n",
        "    \"If you had met me at that time, we would have gone out to eat.\",\n",
        "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
        "]\n",
        "\n",
        "# Preprocess and translate\n",
        "src_lang, tgt_lang = \"eng_Latn\", \"tel_Telu\"  # Adjust target language as needed\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "# Preprocess input sentences\n",
        "batch = ip.preprocess_batch(input_sentences, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "# Tokenize the sentences and generate input encodings\n",
        "inputs = tokenizer(\n",
        "    batch,\n",
        "    truncation=True,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\",\n",
        ").to(DEVICE)  # Move inputs to the same device as the model\n",
        "\n",
        "# Generate translations using the model\n",
        "with torch.no_grad():\n",
        "    generated_tokens = lora_model.generate(\n",
        "        **inputs,\n",
        "        use_cache=True,\n",
        "        min_length=0,\n",
        "        max_length=256,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "# Decode the generated tokens into text\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    generated_tokens = tokenizer.batch_decode(\n",
        "        generated_tokens.detach().cpu().tolist(),\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "\n",
        "# Postprocess the translations\n",
        "translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "# Print the results\n",
        "for input_sentence, translation in zip(input_sentences, translations):\n",
        "    print(f\"Source: {input_sentence}\")\n",
        "    print(f\"Translation: {translation}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7BXZfXa7VyP",
        "outputId": "422602c5-f38b-4955-c5d1-4bce0e24d544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.4.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Source: When I was young, I used to go to the park every day.\n",
            "Translation: நான் இளமையாக இருந்தபோது, நான் தினமும் பூங்காவுக்குச் செல்வது வழக்கம். \n",
            "\n",
            "Source: We watched a new movie last week, which was very inspiring.\n",
            "Translation: கடந்த வாரம் ஒரு புதிய திரைப்படத்தைப் பார்த்தோம், அது மிகவும் ஊக்கமளிக்கும் வகையில் இருந்தது. \n",
            "\n",
            "Source: If you had met me at that time, we would have gone out to eat.\n",
            "Translation: அந்த நேரத்தில் நீங்கள் என்னைச் சந்தித்திருந்தால், நாங்கள் சாப்பிட வெளியே சென்றிருப்போம். \n",
            "\n",
            "Source: My friend has invited me to his birthday party, and I will give him a gift.\n",
            "Translation: என் நண்பர் என்னை அவரது பிறந்த நாள் விருந்துக்கு அழைத்தார், நான் அவருக்கு ஒரு பரிசைக் கொடுப்பேன். \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "\n",
        "# Load the base model\n",
        "base_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"  # Change this as per your use case\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_ckpt_dir, trust_remote_code=True)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_ckpt_dir, trust_remote_code=True)\n",
        "\n",
        "# Load the LoRA model\n",
        "lora_ckpt_dir = \"/content/fine_tuned_model_tamil\"  # Path to your fine-tuned model directory\n",
        "lora_model = PeftModel.from_pretrained(base_model, lora_ckpt_dir)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lora_model.to(DEVICE)\n",
        "\n",
        "# Test translation with your model\n",
        "input_sentences = [\n",
        "    \"When I was young, I used to go to the park every day.\",\n",
        "    \"We watched a new movie last week, which was very inspiring.\",\n",
        "    \"If you had met me at that time, we would have gone out to eat.\",\n",
        "    \"My friend has invited me to his birthday party, and I will give him a gift.\",\n",
        "]\n",
        "\n",
        "# Preprocess and translate\n",
        "src_lang, tgt_lang = \"eng_Latn\", \"tam_Taml\"  # Adjust target language as needed\n",
        "ip = IndicProcessor(inference=True)\n",
        "\n",
        "# Preprocess input sentences\n",
        "batch = ip.preprocess_batch(input_sentences, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "# Tokenize the sentences and generate input encodings\n",
        "inputs = tokenizer(\n",
        "    batch,\n",
        "    truncation=True,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\",\n",
        ").to(DEVICE)  # Move inputs to the same device as the model\n",
        "\n",
        "# Generate translations using the model\n",
        "with torch.no_grad():\n",
        "    generated_tokens = lora_model.generate(\n",
        "        **inputs,\n",
        "        use_cache=True,\n",
        "        min_length=0,\n",
        "        max_length=256,\n",
        "        num_beams=5,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "# Decode the generated tokens into text\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    generated_tokens = tokenizer.batch_decode(\n",
        "        generated_tokens.detach().cpu().tolist(),\n",
        "        skip_special_tokens=True,\n",
        "        clean_up_tokenization_spaces=True,\n",
        "    )\n",
        "\n",
        "# Postprocess the translations\n",
        "translations = ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n",
        "\n",
        "# Print the results\n",
        "for input_sentence, translation in zip(input_sentences, translations):\n",
        "    print(f\"Source: {input_sentence}\")\n",
        "    print(f\"Translation: {translation}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrN7H4Ry3rfw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B041IYjv3rcu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aexs838TG6uQ"
      },
      "outputs": [],
      "source": [
        "#v1\n",
        "%%capture\n",
        "!git clone https://github.com/AI4Bharat/IndicTrans2.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtCFppWkG6uQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cd /content/IndicTrans2/huggingface_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSnvpzsQG6uQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n",
        "!python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "!python3 -m pip install bitsandbytes scipy accelerate datasets\n",
        "!python3 -m pip install sentencepiece\n",
        "\n",
        "!git clone https://github.com/VarunGumma/IndicTransToolkit.git\n",
        "%cd IndicTransToolkit\n",
        "!python3 -m pip install --editable ./\n",
        "%cd ..\n",
        "\n",
        "#restart session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk5799UA2Euz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1af104c-c2d2-46b7-eaf1-35183909d5dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: fine_tuned_model_tamil\n",
            "Files extracted to: fine_tuned_model_telugu\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_file(zip_file_path, extract_to_path):\n",
        "    \"\"\"Unzips a ZIP file to a specified directory.\"\"\"\n",
        "    os.makedirs(extract_to_path, exist_ok=True)  # Create extraction directory\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to_path)  # Extract all contents\n",
        "        print(f\"Files extracted to: {extract_to_path}\")\n",
        "    except (zipfile.BadZipFile, FileNotFoundError):\n",
        "        print(\"Error: Invalid ZIP file or file not found.\")\n",
        "\n",
        "# Example usage\n",
        "unzip_file('/content/fine_tuned_model_tamil.zip', 'fine_tuned_model_tamil')\n",
        "unzip_file('/content/fine_tuned_model_telugu.zip', 'fine_tuned_model_telugu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQi0x-mlWhxC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "quantization = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSQ3SmlCG7Rr",
        "outputId": "55e50ea2-0e77-400a-ba50-a71007c5c78b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting fitz\n",
            "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.12-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting peft\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (10.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Collecting configobj (from fitz)\n",
            "  Downloading configobj-5.0.9.tar.gz (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.5/101.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting configparser (from fitz)\n",
            "  Downloading configparser-7.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.10/dist-packages (from fitz) (0.22.0)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from fitz) (5.2.1)\n",
            "Collecting nipype (from fitz)\n",
            "  Downloading nipype-1.8.6-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fitz) (2.2.2)\n",
            "Collecting pyxnat (from fitz)\n",
            "  Downloading pyxnat-1.6.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fitz) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fitz) (2024.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2->fitz) (3.2.0)\n",
            "Collecting prov>=1.5.2 (from nipype->fitz)\n",
            "  Downloading prov-2.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: pydot>=1.2.3 in /usr/local/lib/python3.10/dist-packages (from nipype->fitz) (3.0.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-7.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting simplejson>=3.8.0 (from nipype->fitz)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting traits!=5.0,<6.4,>=4.6 (from nipype->fitz)\n",
            "  Downloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting etelemetry>=0.2.0 (from nipype->fitz)\n",
            "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting looseversion (from nipype->fitz)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: lxml>=4.3 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (4.9.4)\n",
            "Requirement already satisfied: pathlib>=1.0 in /usr/local/lib/python3.10/dist-packages (from pyxnat->fitz) (1.0.1)\n",
            "Collecting ci-info>=0.2 (from etelemetry>=0.2.0->nipype->fitz)\n",
            "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Collecting rdflib>=5.0.0 (from nipype->fitz)\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->fitz) (1.16.0)\n",
            "Collecting isodate<0.7.0,>=0.6.0 (from rdflib>=5.0.0->nipype->fitz)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading PyMuPDF-1.24.12-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (19.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
            "Downloading nipype-1.8.6-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
            "Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading traits-6.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: configobj\n",
            "  Building wheel for configobj (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.9-py2.py3-none-any.whl size=35615 sha256=de9ec26267875dc1c04b8a7d5c09be9576f317fb65140e274cef9047c372de43\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/6c/03/6c5e3cf1a6e4b9e2fc5c4409be4abc5a8268bd9c878739cb32\n",
            "Successfully built configobj\n",
            "Installing collected packages: looseversion, watchdog, traits, simplejson, pytesseract, pymupdf, pdf2image, isodate, configparser, configobj, ci-info, rdflib, pyxnat, pydeck, etelemetry, prov, nipype, streamlit, peft, fitz\n",
            "Successfully installed ci-info-0.3.0 configobj-5.0.9 configparser-7.1.0 etelemetry-0.3.1 fitz-0.0.1.dev2 isodate-0.6.1 looseversion-1.3.0 nipype-1.8.6 pdf2image-1.17.0 peft-0.13.2 prov-2.0.1 pydeck-0.9.1 pymupdf-1.24.12 pytesseract-0.3.13 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.3 streamlit-1.39.0 traits-6.3.2 watchdog-5.0.3\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 1s (251 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,373 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123652 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "--2024-10-27 09:43:36--  https://github.com/google/fonts/raw/main/ofl/tirotelugu/TiroTelugu-Regular.ttf\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/google/fonts/main/ofl/tirotelugu/TiroTelugu-Regular.ttf [following]\n",
            "--2024-10-27 09:43:36--  https://raw.githubusercontent.com/google/fonts/main/ofl/tirotelugu/TiroTelugu-Regular.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 635592 (621K) [application/octet-stream]\n",
            "Saving to: ‘TiroTelugu-Regular.ttf’\n",
            "\n",
            "TiroTelugu-Regular. 100%[===================>] 620.70K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-10-27 09:43:37 (11.4 MB/s) - ‘TiroTelugu-Regular.ttf’ saved [635592/635592]\n",
            "\n",
            "--2024-10-27 09:43:37--  https://github.com/google/fonts/raw/main/ofl/tirotamil/TiroTamil-Regular.ttf\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/google/fonts/main/ofl/tirotamil/TiroTamil-Regular.ttf [following]\n",
            "--2024-10-27 09:43:37--  https://raw.githubusercontent.com/google/fonts/main/ofl/tirotamil/TiroTamil-Regular.ttf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 197432 (193K) [application/octet-stream]\n",
            "Saving to: ‘TiroTamil-Regular.ttf’\n",
            "\n",
            "TiroTamil-Regular.t 100%[===================>] 192.80K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-10-27 09:43:37 (5.20 MB/s) - ‘TiroTamil-Regular.ttf’ saved [197432/197432]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image pytesseract fitz pymupdf datasets torch transformers streamlit peft\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr\n",
        "!wget https://github.com/google/fonts/raw/main/ofl/tirotelugu/TiroTelugu-Regular.ttf\n",
        "!wget https://github.com/google/fonts/raw/main/ofl/tirotamil/TiroTamil-Regular.ttf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WetRc7rg9WE2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save the Streamlit script in Colab\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(\"\"\"\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from IndicTransToolkit import IndicProcessor\n",
        "import pymupdf as fitz  # PyMuPDF for PDF manipulation\n",
        "from pdf2image import convert_from_path  # Convert PDF to images\n",
        "import pytesseract  # for OCR\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import os\n",
        "import time\n",
        "from peft import PeftModel  # Import the PeftModel for LoRA\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Initialize the base model and tokenizer\n",
        "base_ckpt_dir = \"ai4bharat/indictrans2-en-indic-dist-200M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_ckpt_dir, trust_remote_code=True)\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_ckpt_dir, trust_remote_code=True)\n",
        "\n",
        "# Set the path to Tesseract\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "# Function to batch translate\n",
        "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n",
        "    translations = []\n",
        "    for i in range(0, len(input_sentences), BATCH_SIZE):\n",
        "        batch = input_sentences[i : i + BATCH_SIZE]\n",
        "\n",
        "        # Preprocess the batch and extract entity mappings\n",
        "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
        "\n",
        "        # Tokenize the batch and generate input encodings\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            truncation=True,\n",
        "            padding=\"longest\",\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate translations using the model\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **inputs,\n",
        "                use_cache=True,\n",
        "                min_length=0,\n",
        "                max_length=256,\n",
        "                num_beams=5,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens into text\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            generated_tokens = tokenizer.batch_decode(\n",
        "                generated_tokens.detach().cpu().tolist(),\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "\n",
        "        # Postprocess the translations, including entity replacement\n",
        "        translations += [text.replace('\"', '').strip() for text in ip.postprocess_batch(generated_tokens, lang=tgt_lang)]\n",
        "\n",
        "        del inputs\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return translations\n",
        "\n",
        "# Define the main translation and PDF editing class\n",
        "class PDFTranslator:\n",
        "    def __init__(self, original_language, target_language, pdf_path, DPI=150):\n",
        "        self.original_language = original_language\n",
        "        self.target_language = target_language\n",
        "        self.pdf_path = pdf_path\n",
        "        self.doc = fitz.open(pdf_path)  # Open the PDF with PyMuPDF\n",
        "        self.pages = convert_from_path(pdf_path, DPI)  # Convert PDF pages to images\n",
        "        self.start_time = time.time()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.ip = IndicProcessor(inference=True)\n",
        "\n",
        "        # Load appropriate LoRA model and font based on target language\n",
        "        if self.target_language == 'tel_Telu':\n",
        "            lora_ckpt_dir = \"/content/fine_tuned_model_telugu\"\n",
        "            self.font_path = \"TiroTelugu-Regular.ttf\"\n",
        "        elif self.target_language == 'tam_Taml':\n",
        "            lora_ckpt_dir = \"/content/fine_tuned_model_tamil\"\n",
        "            self.font_path = \"/content/TiroTamil-Regular.ttf\"\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported target language\")\n",
        "\n",
        "        # Load the LoRA model\n",
        "        self.model = PeftModel.from_pretrained(base_model, lora_ckpt_dir)\n",
        "        self.model.to(DEVICE)\n",
        "\n",
        "    def main(self):\n",
        "        for i, page in enumerate(self.pages):\n",
        "            self.process_page(image=page, page_num=i)\n",
        "        pdf_name, _ = os.path.splitext(self.pdf_path)\n",
        "        self.doc.save(f\"{pdf_name}_{self.target_language}.pdf\", garbage=4, deflate=True)\n",
        "        end_time = time.time()\n",
        "        return f\"{pdf_name}_{self.target_language}.pdf\", end_time - self.start_time\n",
        "\n",
        "    def process_page(self, image, page_num):\n",
        "        # Get image size\n",
        "        img_width, img_height = image.size\n",
        "\n",
        "        # OCR the image\n",
        "        ocr_result = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "        # Store detected text paragraphs with their bounding boxes\n",
        "        paragraphs = []\n",
        "        current_paragraph_text = ''\n",
        "        paragraph_bbox = {'left': float('inf'), 'top': float('inf'), 'right': 0, 'bottom': 0}\n",
        "\n",
        "        for i in range(len(ocr_result['text'])):\n",
        "            text = ocr_result['text'][i].strip()\n",
        "            if text:\n",
        "                # If the text is valid, accumulate it in the current paragraph\n",
        "                current_paragraph_text += text + ' '\n",
        "                left, top, width, height = ocr_result['left'][i], ocr_result['top'][i], ocr_result['width'][i], ocr_result['height'][i]\n",
        "                paragraph_bbox['left'] = min(paragraph_bbox['left'], left)\n",
        "                paragraph_bbox['top'] = min(paragraph_bbox['top'], top)\n",
        "                paragraph_bbox['right'] = max(paragraph_bbox['right'], left + width)\n",
        "                paragraph_bbox['bottom'] = max(paragraph_bbox['bottom'], top + height)\n",
        "\n",
        "        if current_paragraph_text:\n",
        "            # Add the last paragraph\n",
        "            paragraphs.append((current_paragraph_text, paragraph_bbox))\n",
        "\n",
        "        # Load the corresponding PDF page\n",
        "        pdf_page = self.doc.load_page(page_num)\n",
        "\n",
        "        # Iterate through paragraphs and translate them\n",
        "        for paragraph, bbox in paragraphs:\n",
        "            translated_text = batch_translate([paragraph], self.original_language, self.target_language, self.model, self.tokenizer, self.ip)[0]\n",
        "\n",
        "            # Convert bounding box to PDF coordinates\n",
        "            rect = fitz.Rect(\n",
        "                bbox['left'] / img_width * pdf_page.rect.width,\n",
        "                bbox['top'] / img_height * pdf_page.rect.height,\n",
        "                bbox['right'] / img_width * pdf_page.rect.width,\n",
        "                bbox['bottom'] / img_height * pdf_page.rect.height\n",
        "            )\n",
        "\n",
        "            # Create an image with the translated text\n",
        "            text_img = self.create_text_image(translated_text, rect.width, rect.height)\n",
        "\n",
        "            # Save the text image\n",
        "            text_img_path = 'temp_text_image.png'\n",
        "            try:\n",
        "                text_img.save(text_img_path)\n",
        "\n",
        "                # Load the image into PyMuPDF and overlay it on the PDF\n",
        "                pdf_page.insert_image(rect, filename=text_img_path)\n",
        "\n",
        "            finally:\n",
        "                # Remove the temporary text image file only if it was saved\n",
        "                if os.path.exists(text_img_path):\n",
        "                    os.remove(text_img_path)\n",
        "\n",
        "    def create_text_image(self, text, max_width, max_height):\n",
        "        # Create an image with the maximum width and height\n",
        "        text_img = Image.new('RGB', (int(max_width), int(max_height)), (255, 255, 255))\n",
        "        draw = ImageDraw.Draw(text_img)\n",
        "        font_size = 16\n",
        "        font = ImageFont.truetype(self.font_path, size=font_size)  # Using the correct font for the target language\n",
        "\n",
        "        # Split text into lines that fit within the width constraint\n",
        "        lines = []\n",
        "        words = text.split(' ')\n",
        "        line = ''\n",
        "        for word in words:\n",
        "            test_line = f\"{line} {word}\".strip()\n",
        "            # Use textbbox to get the bounding box of the text\n",
        "            test_bbox = draw.textbbox((0, 0), test_line, font=font)\n",
        "            test_width = test_bbox[2] - test_bbox[0]\n",
        "            if test_width <= max_width:\n",
        "                line = test_line\n",
        "            else:\n",
        "                lines.append(line)\n",
        "                line = word\n",
        "        lines.append(line)\n",
        "\n",
        "        # Calculate text height using textbbox\n",
        "        text_height = sum(draw.textbbox((0, 0), line, font=font)[3] - draw.textbbox((0, 0), line, font=font)[1] for line in lines)\n",
        "\n",
        "        # Adjust the image height if needed\n",
        "        if text_height > max_height:\n",
        "            text_img = Image.new('RGB', (int(max_width), int(max_height)), (255, 255, 255))\n",
        "            draw = ImageDraw.Draw(text_img)\n",
        "\n",
        "        # Draw the text on the image\n",
        "        y = 0\n",
        "        for line in lines:\n",
        "            draw.text((0, y), line, font=font, fill=(0, 0, 0))\n",
        "            y += draw.textbbox((0, 0), line, font=font)[3] - draw.textbbox((0, 0), line, font=font)[1]\n",
        "\n",
        "        return text_img\n",
        "\n",
        "# Streamlit Application\n",
        "st.title('PDF Translation App')\n",
        "\n",
        "# Upload PDF\n",
        "uploaded_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n",
        "\n",
        "# Source and Target Language Inputs\n",
        "src_lang = st.text_input(\"Source Language Code (e.g., eng_Latn)\", 'eng_Latn')\n",
        "tgt_lang = st.text_input(\"Target Language Code (e.g., tel_Telu for Telugu, tam_Taml for Tamil)\", 'tel_Telu')\n",
        "\n",
        "if st.button(\"Translate\"):\n",
        "    if uploaded_file is not None:\n",
        "        pdf_path = \"/content/\" + uploaded_file.name\n",
        "        with open(pdf_path, 'wb') as f:\n",
        "            f.write(uploaded_file.read())\n",
        "\n",
        "        translator = PDFTranslator(src_lang, tgt_lang, pdf_path)\n",
        "        output_pdf, time_taken = translator.main()\n",
        "\n",
        "        st.success(f\"Translation completed in {time_taken:.2f} seconds!\")\n",
        "        st.download_button(\"Download Translated PDF\", open(output_pdf, \"rb\"), file_name=output_pdf)\n",
        "    else:\n",
        "        st.error(\"Please upload a PDF file.\")\n",
        "\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NClwjlsfO0ze",
        "outputId": "23d0d2ef-d1c8-4dcb-8ba7-5ff28d2bd4a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.188.97.127\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFzFZp-hLxKQ",
        "outputId": "53b075a7-7a12-4ccc-d18b-2cdc12229732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 2s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues, run:\n",
            "  npm audit fix\n",
            "\n",
            "Run `npm audit` for details.\n",
            "your url is: https://open-eagles-behave.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npm install localtunnel\n",
        "\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "\n",
        "!npx localtunnel --port 8501\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
